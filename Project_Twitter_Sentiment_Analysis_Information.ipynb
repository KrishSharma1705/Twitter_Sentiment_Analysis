{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yYDXllA4Ppr"
      },
      "source": [
        "Twitter Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "_0A4Z0_-3cFd",
        "outputId": "9b70c950-b4e7-4b9a-ac19-930c02769a5d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'In this project, we try to implement an NLP Twitter sentiment analysis model that helps to overcome the challenges of sentiment classification of tweets.\\nWe will be classifying the tweets into positive or negative sentiments. \\nThe necessary details regarding the dataset involving the Twitter sentiment analysis project are:'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''In this project, we try to implement an NLP Twitter sentiment analysis model that helps to overcome the challenges of sentiment classification of tweets.\n",
        "We will be classifying the tweets into positive or negative sentiments.\n",
        "The necessary details regarding the dataset involving the Twitter sentiment analysis project are:'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "JvAJ3OBP4otG",
        "outputId": "a7c3d888-78c4-465b-9027-c8b6c993cb5c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The dataset provided is the Sentiment140 Dataset which consists of 1,600,000 tweets that \\nhave been extracted using the Twitter API. The various columns present in this Twitter data are:'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''The dataset provided is the Sentiment140 Dataset which consists of 1,600,000 tweets that\n",
        "have been extracted using the Twitter API. The various columns present in this Twitter data are:'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "s9s_arGb42ZY"
      },
      "outputs": [],
      "source": [
        "#label: the polarity of the tweet (positive or negative)\n",
        "#time: Unique id of the tweet\n",
        "#date: the date of the tweet\n",
        "#query: It refers to the query. If no such query exists, then it is NO QUERY.\n",
        "#username: It refers to the name of the user that tweeted\n",
        "#text: It refers to the text of the tweet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7seLHk365dlY"
      },
      "source": [
        "Twitter Sentiment Analysis: Project Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DcfWjoaj42cj"
      },
      "outputs": [],
      "source": [
        "#The various steps involved in the Machine Learning Pipeline are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qBUq03195j_3"
      },
      "outputs": [],
      "source": [
        "#Import Necessary Dependencies\n",
        "#Read and Load the Dataset\n",
        "#Exploratory Data Analysis\n",
        "#Data Visualization of Target Variables\n",
        "#Data Preprocessing\n",
        "#Splitting our data into Train and Test sets\n",
        "#Function for Model Evaluation\n",
        "#Model Building\n",
        "#Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Pp9DwAgy5kCs"
      },
      "outputs": [],
      "source": [
        "#Step-1: Import the Necessary Dependencies\n",
        "#Step-2: Read and Load the Dataset\n",
        "#Step-3: Exploratory Data Analysis\n",
        "#Step-4: Data Visualization of Target Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "i6EiOn2JIhHB",
        "outputId": "300f862a-8396-4bec-98ab-e6e1c97219ff"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Step-5: Data Preprocessing\\nIn the above-given problem statement, before training the model, we performed various pre-processing steps on the dataset that mainly dealt with removing stopwords, removing special characters like emojis, hashtags, etc. The text document is then converted into lowercase for better generalization.\\n\\nSubsequently, the punctuations were cleaned and removed, thereby reducing the unnecessary noise from the dataset. After that, we also removed the repeating characters from the words along with removing the URLs as they do not have any significant importance.\\n\\nAt last, we then performed Stemming(reducing the words to their derived stems) and Lemmatization(reducing the derived words to their root form, known as lemma) for better results.\\n\\n5.1: Selecting the text and Target column for our further analysis.'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''Step-5: Data Preprocessing\n",
        "In the above-given problem statement, before training the model, we performed various pre-processing steps on the dataset that mainly dealt with removing stopwords, removing special characters like emojis, hashtags, etc. The text document is then converted into lowercase for better generalization.\n",
        "\n",
        "Subsequently, the punctuations were cleaned and removed, thereby reducing the unnecessary noise from the dataset. After that, we also removed the repeating characters from the words along with removing the URLs as they do not have any significant importance.\n",
        "\n",
        "At last, we then performed Stemming(reducing the words to their derived stems) and Lemmatization(reducing the derived words to their root form, known as lemma) for better results.\n",
        "\n",
        "5.1: Selecting the text and Target column for our further analysis.'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GTTzju7k5kFj"
      },
      "outputs": [],
      "source": [
        "#Plot a cloud of words for negative tweets\n",
        "\n",
        "#Plot a cloud of words for positive tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQO7Yk7cJdWx"
      },
      "outputs": [],
      "source": [
        "#Step-6: Splitting Our Data Into Train and Test Subsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDQzzjPfJdaD"
      },
      "outputs": [],
      "source": [
        "#Step-7: Transforming the Dataset Using TF-IDF Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3FcAXdcJ3l8"
      },
      "outputs": [],
      "source": [
        "'''Step-8: Function for Model Evaluation\n",
        "After training the model, we then apply the evaluation measures to check how the model is performing. Accordingly,\n",
        "we use the following evaluation parameters to check the performance of the models respectively:'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IgZ1x0K8J8jb"
      },
      "outputs": [],
      "source": [
        "#Accuracy Score\n",
        "#Confusion Matrix with Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsRcLdUqJ8mu"
      },
      "outputs": [],
      "source": [
        "#Step-9: Model Building\n",
        "'''In the problem statement, we have used three different models respectively :\n",
        "\n",
        "Bernoulli Naive Bayes Classifier\n",
        "SVC\n",
        "Logistic Regression\n",
        "The idea behind choosing these models is that we want to try all the classifiers on the dataset ranging from simple ones to complex models,\n",
        "and then try to find out the one which gives the best performance among them.'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJM0ULniLSH7"
      },
      "source": [
        "STEP 4: Transforming the Dataset Using TF-IDF Vectorizer\n",
        "\n",
        "CODE USED: from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vect = CountVectorizer(ngram_range=(1,2)).fit(df['newname'])\n",
        "\n",
        "feature_names = vect.get_feature_names_out()\n",
        "\n",
        "print('Number of features: {}\\n'.format(len(feature_names)))\n",
        "\n",
        "print('First 20 features: \\n{}'.format(feature_names[:20]))\n",
        "\n",
        "X = df['newname']\n",
        "\n",
        "Y = df['target']\n",
        "\n",
        "X = vect.transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GQ7meq2LWER"
      },
      "source": [
        "STEP 11: SPLITTING THE DATA FOR TRAINING AND TESTING PURPOSE\n",
        "\n",
        "CODE USED: X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state =42)\n",
        "\n",
        "print('Size of X_train:',(X_train.shape))\n",
        "\n",
        "print('Size of y_train:',(y_train.shape))\n",
        "\n",
        "print('Size of X_test:',(X_test.shape))\n",
        "\n",
        "print('Size of y_test:',(y_test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvwalmypLcx9"
      },
      "source": [
        "TEP 12: BUILDING MODEL(applied 2 machine learning algorithms LogisticRegression & LinearSVC)\n",
        "\n",
        "MODEL 1 : LogisticRegression CODE USED:\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "logreg.fit(X_train,y_train)\n",
        "\n",
        "logreg_pred = logreg.predict(X_test)\n",
        "\n",
        "logreg_acc =accuracy_score(logreg_pred,y_test)\n",
        "\n",
        "print('Test accuracy : {:.2f}%'.format(logreg_acc*100))\n",
        "\n",
        "print(confusion_matrix(y_test, logreg_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(classification_report(y_test, logreg_pred))\n",
        "\n",
        "we get an accuracy of 81.58% using LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGo6qSLbLk6I"
      },
      "source": [
        "MODEL 13: LinearSVC Then applying second model by using following code:\n",
        "\n",
        "SVCmodel = LinearSVC()\n",
        "\n",
        "SVCmodel.fit(X_train,y_train)\n",
        "\n",
        "svc_pred = SVCmodel.predict(X_test)\n",
        "\n",
        "svc_acc = accuracy_score(svc_pred,y_test)\n",
        "\n",
        "print(\"test accuracy : {:.2f}%\".format(svc_acc*100))\n",
        "\n",
        "We get an accuracy of 80.58% using LinearSVC.\n",
        "\n",
        "print(confusion_matrix(y_test,svc_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(classification_report(y_test,svc_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-yuPXuTLmyn"
      },
      "source": [
        "CONCLUSION: while doing sentiment analysis of this dataset of 1600000 tweets in twitter we classified postive and negative tweets and demonstratewd them on graph by use of matplotlib and seaborn. We applied two models after the preprocessing & exploratory data analysis That is LogisticRegression & LinearSVC. We are getting an accuracy of 81.75% in classifying positive & negative tweets using LogisticRegression while we are getting an accuracy of 80.93% in classifying positive & negative tweets using Linear SVC model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
